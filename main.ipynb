{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5101edfb",
   "metadata": {},
   "source": [
    "# ALL THE IMPORTS \n",
    " - has all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DDIMScheduler\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.optim import Adam\n",
    "\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "\n",
    "from Zstar.d_utils import ZstarAudioPipeline\n",
    "\n",
    "# import ptp_utils\n",
    "\n",
    "from helpers import load_audio_to_numpy, load_audio, get_audio_files, load_and_view_audio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7a9e9",
   "metadata": {},
   "source": [
    "# CONSTANTS\n",
    "\n",
    "-   Constant declerations.\n",
    "-   This is also where i replace the scheduler with DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TARGET_SR = 44100\n",
    "CLIP_DURATION_SECONDS = 5.0\n",
    "TARGET_SAMPLES = int(TARGET_SR * CLIP_DURATION_SECONDS)\n",
    "TOTAL_STEPS = 100\n",
    "GUIDANCE_SCALE = 7.5\n",
    "AUDIO_MODEL_PATH = \"./stable-audio-open-1.0\"\n",
    "SEED = 9999\n",
    "# START_STEP = 5\n",
    "# END_STEP = 15\n",
    "# LAYER_INDEX = [14, 16, 18, 20, 22, 24]\n",
    "\n",
    "# Setup\n",
    "seed_everything(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Scheduler\n",
    "inversion_scheduler = DDIMScheduler.from_pretrained(\n",
    "    AUDIO_MODEL_PATH,\n",
    "    subfolder=\"scheduler\",\n",
    "    prediction_type=\"v_prediction\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = ZstarAudioPipeline.from_pretrained(\n",
    "    AUDIO_MODEL_PATH, scheduler=inversion_scheduler\n",
    ").to(device)\n",
    "# Replace inversion scheduler\n",
    "model.inversion_scheduler = inversion_scheduler\n",
    "model.inversion_scheduler.timesteps = model.inversion_scheduler.timesteps.to(device)\n",
    "model.inversion_scheduler.alphas_cumprod = model.inversion_scheduler.alphas_cumprod.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a973d2",
   "metadata": {},
   "source": [
    "# Null Inversion Class \n",
    "\n",
    "The content audio gets passed to the below class where the invert method first calls the ddim_inversion method and then null_optimisation for null prompting later.\n",
    "\n",
    "This was taken from the Zero shot paper implementation adapted to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullInversion:\n",
    "    def __init__(self, model, num_ddim_steps=50):\n",
    "        self.model = model\n",
    "        self.tokenizer = model.tokenizer\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "        self._ddim_scheduler = DDIMScheduler.from_pretrained(AUDIO_MODEL_PATH, subfolder=\"scheduler\")\n",
    "        self._ddim_scheduler.set_timesteps(num_ddim_steps)\n",
    "        self._ddim_scheduler.timesteps = self._ddim_scheduler.timesteps.to(self.model.device)\n",
    "        # 3. Explicitly move its internal `alphas_cumprod` tensor to the device\n",
    "        self._ddim_scheduler.alphas_cumprod = self._ddim_scheduler.alphas_cumprod.to(self.model.device)\n",
    "\n",
    "    def prev_step(self, noise_pred: torch.FloatTensor, timestep: int, latents: torch.FloatTensor):\n",
    "        sched = self._ddim_scheduler\n",
    "        prev_t = timestep - sched.config.num_train_timesteps // sched.num_inference_steps\n",
    "        alpha_t = sched.alphas_cumprod[timestep]\n",
    "        alpha_prev = (sched.alphas_cumprod[prev_t] if prev_t >= 0 else sched.final_alpha_cumprod)\n",
    "        beta_t = 1 - alpha_t\n",
    "        pred_x0 = (latents - beta_t**0.5 * noise_pred) / alpha_t**0.5\n",
    "        dir_prev = (1 - alpha_prev)**0.5 * noise_pred\n",
    "        z_prev = alpha_prev**0.5 * pred_x0 + dir_prev\n",
    "        return z_prev, pred_x0\n",
    "\n",
    "    def next_step(self, noise_pred: torch.FloatTensor, t: int, latents: torch.FloatTensor):\n",
    "        # This is the DDIM inversion logic to ADD noise (t -> t+1)\n",
    "        prev_t = t \n",
    "        next_t = t + self._ddim_scheduler.config.num_train_timesteps // self._ddim_scheduler.num_inference_steps\n",
    "        alpha_t = self._ddim_scheduler.alphas_cumprod[prev_t]\n",
    "        alpha_next = self._ddim_scheduler.alphas_cumprod[next_t] if next_t < len(self._ddim_scheduler.alphas_cumprod) else self._ddim_scheduler.final_alpha_cumprod\n",
    "        beta_t = 1 - alpha_t\n",
    "        pred_x0 = (latents - beta_t**0.5 * noise_pred) / alpha_t**0.5\n",
    "        dir_next = (1 - alpha_next)**0.5 * noise_pred\n",
    "        return alpha_next**0.5 * pred_x0 + dir_next\n",
    "\n",
    "    def get_noise_pred_single(self, latents: torch.Tensor, t: Union[int, torch.Tensor], context: torch.Tensor, global_states: Optional[torch.Tensor] = None):\n",
    "        if not isinstance(t, torch.Tensor): t = torch.tensor([t], device=self.model.device, dtype=torch.long)\n",
    "        elif t.dim() == 0: t = t.unsqueeze(0).to(self.model.device)\n",
    "        gs = global_states if global_states is not None else getattr(self, \"global_states\", None)\n",
    "        noise_out = self.model.transformer(latents, t, encoder_hidden_states=context, global_hidden_states=gs, return_dict=False)[0]\n",
    "        return noise_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def audio2latent(self, audio: Union[torch.Tensor, np.ndarray]):\n",
    "        if isinstance(audio, np.ndarray):\n",
    "            audio = torch.from_numpy(audio).unsqueeze(0).to(self.model.device, dtype=torch.float32) # Ensure correct dtype\n",
    "        if audio.dim() == 2:\n",
    "            audio = audio.unsqueeze(1)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # First, get the VAE output object\n",
    "        vae_output = self.model.vae.encode(audio)\n",
    "        # Then, access the latent tensor and apply .float() to it\n",
    "        latents = vae_output.latent_dist.mean.float()\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2audio(self, latents: torch.Tensor, return_type: str = \"pt\"):\n",
    "        wav = self.model.vae.decode(latents / 0.18215)[\"sample\"].clamp(-1, 1)\n",
    "        return wav if return_type == \"pt\" else wav[0, 0].cpu().numpy()\n",
    "\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uc = self.model.tokenizer([\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "        uc_emb = self.model.text_encoder(uc.input_ids.to(self.model.device))[0]\n",
    "        tc = self.model.tokenizer([prompt], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        cond_emb = self.model.text_encoder(tc.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uc_emb, cond_emb], dim=0)\n",
    "        self.prompt = prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, audio: np.ndarray):\n",
    "        wav_t = torch.from_numpy(audio).permute(1, 0).unsqueeze(0).to(self.model.device, dtype=torch.float32)\n",
    "        cond_emb = self.context.chunk(2, dim=0)[1]\n",
    "        z0 = self.audio2latent(wav_t) * 0.18215\n",
    "        \n",
    "        duration_s = audio.shape[0] / TARGET_SR\n",
    "        ss, es = self.model.encode_duration(0.0, duration_s, self.model.device, do_classifier_free_guidance=False, batch_size=1)\n",
    "        global_states_unprojected = torch.cat([ss, es], dim=-1)\n",
    "        self.global_states = self.model.transformer.global_proj(global_states_unprojected)\n",
    "        \n",
    "        traj = [z0]\n",
    "        z = z0.clone()\n",
    "        for t in tqdm(reversed(self._ddim_scheduler.timesteps), desc=\"DDIM Inversion\"):\n",
    "            noise_pred = self.get_noise_pred_single(z, t, cond_emb, self.global_states)\n",
    "            z = self.next_step(noise_pred, t, z)\n",
    "            traj.append(z)\n",
    "        return self.latent2audio(z0, return_type=\"pt\"), traj\n",
    "\n",
    "    def null_optimization(self, latents: list[torch.Tensor], num_inner_steps: int, epsilon: float):\n",
    "        print(\"null optimisation\")\n",
    "        uncond_emb, cond_emb = self.context.chunk(2, dim=0)\n",
    "        optimized = []\n",
    "        z_cur = latents[-1].clone()\n",
    "        for i, t in enumerate(tqdm(self._ddim_scheduler.timesteps, desc=\"Null-text Optimization\")):\n",
    "            u = uncond_emb.clone().detach().requires_grad_(True)\n",
    "            optimizer = Adam([u], lr=1e-2 * (1 - i / len(latents)))\n",
    "            z_prev_target = latents[len(latents) - i - 2]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                noise_cond = self.get_noise_pred_single(z_cur, t, cond_emb, self.global_states)\n",
    "            for _ in range(num_inner_steps):\n",
    "                noise_uncond = self.get_noise_pred_single(z_cur, t, u, self.global_states)\n",
    "                noise = noise_uncond + GUIDANCE_SCALE * (noise_cond - noise_uncond)\n",
    "                z_prev_est, _ = self.prev_step(noise, t, z_cur)\n",
    "                loss = F.mse_loss(z_prev_est, z_prev_target)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                if loss.item() < epsilon: break\n",
    "            \n",
    "            optimized.append(u.detach())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                noise_uncond_final = self.get_noise_pred_single(z_cur, t, u, self.global_states)\n",
    "                noise_final = noise_uncond_final + GUIDANCE_SCALE * (noise_cond - noise_uncond_final)\n",
    "                z_cur, _ = self.prev_step(noise_final, t, z_cur)\n",
    "        return optimized\n",
    "\n",
    "    def invert(self, audio_path: str, prompt: str, num_inner_steps: int = 10, early_stop_epsilon: float = 1e-5, verbose: bool = False):\n",
    "        self.init_prompt(prompt)\n",
    "        \n",
    "        # Not needed for reconstruction, but to be used later\n",
    "        # ptp_utils.register_attention_control(self.model, None)\n",
    "        \n",
    "        \n",
    "        if verbose: print(\"Running DDIM inversion…\")\n",
    "        wav_gt = load_audio_to_numpy(audio_path)\n",
    "        audio_rec, traj = self.ddim_inversion(wav_gt)\n",
    "        if verbose: print(\"Running null‐text optimization…\")\n",
    "        uncond_list = self.null_optimization(traj, num_inner_steps, early_stop_epsilon)\n",
    "        return (wav_gt, audio_rec), traj, traj[-1], uncond_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5455a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_inversion = NullInversion(model, num_ddim_steps=TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764148b7",
   "metadata": {},
   "source": [
    "# Main Loop\n",
    "\n",
    "- This is where the reconstruction happens, I compare it with the ground truth\n",
    "\n",
    "- This is also where the d_utils class is used, for reconstructions we only use the Invert method from the class and apply it on style data.\n",
    "\n",
    "- Null Inversion class is solely used for content and model invert for style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d23ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_files = get_audio_files(\"content\")\n",
    "style_files = get_audio_files(\"style\")\n",
    "\n",
    "for cpath in content_files:\n",
    "    print(f\"Processing content: {os.path.basename(cpath)}\")\n",
    "    pkl = cpath.rsplit('.',1)[0] + \".pkl\"\n",
    "\n",
    "    if os.path.isfile(pkl):\n",
    "        # Reconstruction for content audio if exists\n",
    "        cont_latents, zT_c, uncond_embeds, (wav_gt, audio_rec_c) = pickle.load(open(pkl,'rb'))\n",
    "\n",
    "        print(\"Ground Truth\")\n",
    "        load_and_view_audio(\"content/content.wav\")\n",
    "        print(\"\\n# ********************\\n\")\n",
    "        print(\"Reconstruction\")\n",
    "        load_and_view_audio(\"content_recon.wav\")\n",
    "\n",
    "    else:\n",
    "        (wav_gt, audio_rec_c), cont_latents, zT_c, uncond_embeds = null_inversion.invert(\n",
    "            cpath, prompt=\"\", verbose=True\n",
    "        )\n",
    "        sf.write('content_recon.wav', audio_rec_c.squeeze().cpu().numpy().T, TARGET_SR)\n",
    "\n",
    "        # Reconstruction for content audio if does not exist\n",
    "        print(\"Ground Truth\")\n",
    "        load_and_view_audio(\"content/content.wav\")\n",
    "        print(\"\\n# ********************\\n\")\n",
    "        print(\"Reconstruction\")\n",
    "        load_and_view_audio(\"content_recon.wav\")\n",
    "\n",
    "        pickle.dump((cont_latents, zT_c, uncond_embeds, (wav_gt, audio_rec_c)), open(pkl,'wb'))\n",
    "        print(\"Saved content reconstruction: {content_recon.wav\")\n",
    "\n",
    "    for style_path in style_files:\n",
    "        print(f\"Processing style: {os.path.basename(style_path)}\")\n",
    "\n",
    "        zT_s, style_latents, z0_s = model.invert(\n",
    "            load_audio(style_path),\n",
    "            prompt=\"\",\n",
    "            num_inference_steps=TOTAL_STEPS,\n",
    "            guidance_scale=7.0,\n",
    "            return_intermediates=True,\n",
    "        )\n",
    "\n",
    "        wav_s_recon = model.latent2audio(z0_s)\n",
    "\n",
    "        # Reconstruction for content audio if exists\n",
    "        sf.write(\"style_recon.wav\", wav_s_recon.squeeze().cpu().numpy().T, TARGET_SR)\n",
    "        load_and_view_audio(\"style_recon.wav\")\n",
    "\n",
    "        print(\"\\n# ********************\\n\")\n",
    "\n",
    "        load_and_view_audio(\"s/style.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
