{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ec13c-c22f-4545-b206-102221aec949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# sys.path.insert(0, os.path.abspath(\"../local_diffusers/src\"))\n",
    "\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "\n",
    "# from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
    "# from diffusers.pipelines.stable_audio.pipeline_stable_audio import StableAudioPipeline\n",
    "\n",
    "from diffusers import StableAudioPipeline, DDIMScheduler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ZstarAudioPipeline(StableAudioPipeline):\n",
    "    def __init__(\n",
    "        self, vae, text_encoder, projection_model, tokenizer, transformer, scheduler\n",
    "    ):\n",
    "        super().__init__(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            projection_model=projection_model,\n",
    "            tokenizer=tokenizer,\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "        )\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.Tensor,  # this is v_t\n",
    "        timestep: int,\n",
    "        sample: torch.Tensor,        # this is x_t\n",
    "    ):\n",
    "        # ---- 1) figure out your \"backward\" timestep decrement ----\n",
    "        dt = self.scheduler.num_train_timesteps // TOTAL_STEPS\n",
    "        t_prev = max(timestep - dt, 0)\n",
    "    \n",
    "        # ---- 2) grab alphas ----\n",
    "        alpha_t      = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_t_prev = (\n",
    "            self.scheduler.alphas_cumprod[t_prev]\n",
    "            if t_prev >= 0\n",
    "            else self.scheduler.final_alpha_cumprod\n",
    "        )\n",
    "        beta_t = 1 - alpha_t\n",
    "    \n",
    "        # ---- 3) predict x0 and eps from v ----\n",
    "        x0  = alpha_t**0.5 * sample - beta_t**0.5 * model_output\n",
    "        eps = alpha_t**0.5 * model_output + beta_t**0.5 * sample\n",
    "    \n",
    "        # ---- 4) step backward ----\n",
    "        prev_sample = alpha_t_prev**0.5 * x0 + (1 - alpha_t_prev)**0.5 * eps\n",
    "    \n",
    "        return prev_sample, x0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2audio(self, latents: torch.FloatTensor, return_type: str = \"np\"):\n",
    "        \"\"\"\n",
    "        Decode a latent back into a waveform.\n",
    "        – latents: (B, C_latent, T_latent)\n",
    "        – returns either a torch.Tensor (B, C_audio, T_audio)\n",
    "          or a numpy array (T_audio, C_audio) for the first batch item.\n",
    "        \"\"\"\n",
    "        # 1) undo the scaling\n",
    "        vae_scale_factor = 0.3704\n",
    "        latents = latents.detach() / vae_scale_factor\n",
    "        # 2) decode\n",
    "        audio = self.vae.decode(latents)[\"sample\"]\n",
    "        # audio is now a torch.Tensor of shape (B, C, T)\n",
    "        if return_type == \"np\":\n",
    "            # clamp into valid [-1,1] range and convert\n",
    "            audio = audio.clamp(-1, 1)\n",
    "            # move channels axis last and pick batch 0\n",
    "            audio_np = audio.cpu().permute(0, 2, 1).numpy()[0]\n",
    "            return audio_np\n",
    "        return audio\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def audio2latent(self, audio: Union[np.ndarray, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Encode a waveform into VAE latents.\n",
    "        – audio: either a np.ndarray (T, C) or torch.Tensor (C, T) / (B, C, T)\n",
    "        – returns a torch.FloatTensor of latents (B, C_latent, T_latent)\n",
    "        \"\"\"\n",
    "        # 1) to torch\n",
    "        # if isinstance(audio, np.ndarray):\n",
    "        #     # assume shape (T, C) → (C, T)\n",
    "        #     audio = torch.from_numpy(audio).permute(1, 0)\n",
    "        # now audio is Tensor of shape (C, T) or (B, C, T)\n",
    "        if audio.dim() == 2:\n",
    "            audio = audio.unsqueeze(0)    # → (1, C, T)\n",
    "        audio = audio.to(self.device).float()\n",
    "        # 2) encode\n",
    "        latents = self.vae.encode(audio)[\"latent_dist\"].mean\n",
    "        # 3) apply the same scale factor\n",
    "        vae_scale_factor = 0.3704\n",
    "        latents = latents * vae_scale_factor\n",
    "        return latents\n",
    "\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt,\n",
    "        batch_size=1,\n",
    "        num_inference_steps=100,\n",
    "        guidance_scale=7.5,\n",
    "        eta=0.0,\n",
    "        latents=None,\n",
    "        unconditioning=None,\n",
    "        uncond_embeddings=None,\n",
    "        neg_prompt=None,\n",
    "        ref_intermediate_latents=None,\n",
    "        return_intermediates=False,\n",
    "        **kwds,\n",
    "    ):\n",
    "        DEVICE = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available(\n",
    "            ) else torch.device(\"cpu\")\n",
    "        )\n",
    "        \n",
    "        if isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        elif isinstance(prompt, str):\n",
    "            if batch_size > 1:\n",
    "                prompt = [prompt] * batch_size\n",
    "\n",
    "\n",
    "        # text embeddings, we pass 64 manually because we changed it in the config from 128 to 64\n",
    "        text_input = self.tokenizer(\n",
    "            prompt, padding=\"max_length\", max_length=64, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(DEVICE))[0]\n",
    "\n",
    "        # We probably never go into this, but check this later\n",
    "        if kwds.get(\"dir\"):\n",
    "            dir = text_embeddings[-2] - text_embeddings[-1]\n",
    "            u, s, v = torch.pca_lowrank(\n",
    "                dir.transpose(-1, -2), q=1, center=True)\n",
    "            text_embeddings[-1] = text_embeddings[-1] + kwds.get(\"dir\") * v\n",
    "\n",
    "        # define initial latents\n",
    "        latents_shape = (batch_size, self.transformer.in_channels, latents.shape[2]) #Todo: remove hardcoded length\n",
    "        \n",
    "        if latents is None:\n",
    "            latents = torch.randn(latents_shape, device=DEVICE)\n",
    "        else:\n",
    "            assert (\n",
    "                latents.shape == latents_shape\n",
    "            ), f\"The shape of input latent tensor {latents.shape} should equal to predefined one.\"\n",
    "\n",
    "        print(latents.shape)\n",
    "\n",
    "\n",
    "        # unconditional embedding for classifier free guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            if neg_prompt:\n",
    "                uc_text = neg_prompt\n",
    "            else:\n",
    "                uc_text = \"\"\n",
    "            if uncond_embeddings is None:\n",
    "                uncond_input = model.tokenizer(\n",
    "                    [\"\"] * batch_size,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                uncond_embeddings_ = model.text_encoder(\n",
    "                    uncond_input.input_ids.to(model.device)\n",
    "                )[0]\n",
    "            else:\n",
    "                uncond_embeddings_ = None\n",
    "\n",
    "        # iterative sampling\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        latents_list = [latents]\n",
    "        pred_x0_list = [latents]\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps, desc=\"DDIM Sampler\")):\n",
    "            if uncond_embeddings_ is None:\n",
    "                context = torch.cat(\n",
    "                    [\n",
    "                        uncond_embeddings[i].expand(*text_embeddings.shape),\n",
    "                        text_embeddings,\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                context = torch.cat([uncond_embeddings_, text_embeddings])\n",
    "            if ref_intermediate_latents is not None:\n",
    "                # note that the batch_size >= 2\n",
    "                style_latents_ref = ref_intermediate_latents[1][-1 - i]\n",
    "                _, content_latents_cur = latents.chunk(2)\n",
    "                content_latents_cur = (\n",
    "                    ref_intermediate_latents[0][-1 - i] * 0.01\n",
    "                    + content_latents_cur * 0.99\n",
    "                )\n",
    "                latents = torch.cat([style_latents_ref, content_latents_cur])\n",
    "\n",
    "            if guidance_scale > 1.0:\n",
    "                model_inputs = torch.cat([latents] * 2)\n",
    "            else:\n",
    "                model_inputs = latents\n",
    "            if unconditioning is not None and isinstance(unconditioning, list):\n",
    "                _, context = context.chunk(2)\n",
    "                context = torch.cat(\n",
    "                    [unconditioning[i].expand(*context.shape), context])\n",
    "            # predict tghe noise\n",
    "            timestep = t.expand(model_inputs.shape[0]).to(self.device)\n",
    "            # print(f\"{context.shape=}\")\n",
    "            # print(f\"{timestep.shape=}\")\n",
    "            # print(f\"{model_inputs.shape=}\")\n",
    "\n",
    "            duration_s = model_inputs.shape[-1] / self.vae.sampling_rate\n",
    "            ss, es = self.encode_duration(0.0, duration_s, self.device, do_classifier_free_guidance=False, batch_size=1)\n",
    "            self.global_states = self.transformer.global_proj(torch.cat([ss, es], dim=-1))\n",
    "            self.global_states = self.global_states.repeat(4,1,1)\n",
    "            # print(\"self.global_states\", self.global_states.shape) # [4, 1, 1536]\n",
    "\n",
    "\n",
    "            noise_pred = self.transformer(\n",
    "                model_inputs, timestep, encoder_hidden_states=context,global_hidden_states=self.global_states\n",
    "            )[0]\n",
    "            \n",
    "            if guidance_scale > 1.0:\n",
    "                noise_pred_uncon, noise_pred_con = noise_pred.chunk(2, dim=0)\n",
    "                noise_pred = noise_pred_uncon + guidance_scale * (\n",
    "                    noise_pred_con - noise_pred_uncon\n",
    "                )\n",
    "\n",
    "\n",
    "            # compute the previous noise sample x_t -> x_t-1\n",
    "            latents, pred_x0 = self.step(noise_pred, t, latents)\n",
    "            latents_list.append(latents)\n",
    "            pred_x0_list.append(pred_x0)\n",
    "\n",
    "\n",
    "        audio = self.latent2audio(latents, return_type=\"pt\")\n",
    "        if return_intermediates:\n",
    "            pred_x0_list = [\n",
    "                self.latent2audio(aud, return_type=\"pt\") for aud in pred_x0_list\n",
    "            ]\n",
    "            latents_list = [\n",
    "                self.latent2audio(aud, return_type=\"pt\") for aud in latents_list\n",
    "            ]\n",
    "            return audio, pred_x0_list, latents_list\n",
    "        return audio\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730930bb-a470-454f-bf3f-9e340e2807e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# sys.path.insert(0, \"local_diffusers/src\")\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "# from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "# from diffusers.schedulers.scheduling_ddim_inverse import DDIMInverseScheduler\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.optim import Adam\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "\n",
    "# from Zstar_test.d_utils import ZstarAudioPipeline\n",
    "# from model import ZstarAudioPipeline\n",
    "from Zstar.Zstar_utils import AttentionBase, register_attention_editor_audio, AttentionStore\n",
    "from Zstar.Zstar import ReweightCrossAttentionControl, ReweightAndStoreAttentionControl\n",
    "# from attention import ReweightCrossAttentionControl\n",
    "# from attention_register import register_attention_editor_diffusers\n",
    "\n",
    "import ptp_utils\n",
    "\n",
    "from helpers import load_audio_to_numpy, load_audio, get_audio_files, load_and_view_audio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4daa228f-b68c-49e3-8262-1171043f8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 9999\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 21.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ZstarAudioPipeline {\n",
       "  \"_class_name\": \"ZstarAudioPipeline\",\n",
       "  \"_diffusers_version\": \"0.35.0.dev0\",\n",
       "  \"_name_or_path\": \"./stable-audio-open-1.0\",\n",
       "  \"projection_model\": [\n",
       "    \"stable_audio\",\n",
       "    \"StableAudioProjectionModel\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DDIMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"T5EncoderModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"T5TokenizerFast\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"StableAudioDiTModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderOobleck\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "TARGET_SR = 44100\n",
    "CLIP_DURATION_SECONDS = 5.0\n",
    "TARGET_SAMPLES = int(TARGET_SR * CLIP_DURATION_SECONDS)\n",
    "TOTAL_STEPS = 100\n",
    "NUM_DDIM_STEPS = 100\n",
    "GUIDANCE_SCALE = 1.1\n",
    "AUDIO_MODEL_PATH = \"./stable-audio-open-1.0\"\n",
    "SEED = 9999\n",
    "START_STEP = 15\n",
    "END_STEP = 50\n",
    "LAYER_INDEX = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "\n",
    "# Setup\n",
    "seed_everything(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = DDIMScheduler.from_pretrained(\n",
    "    AUDIO_MODEL_PATH,\n",
    "    subfolder=\"scheduler\",\n",
    "    prediction_type=\"v_prediction\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = ZstarAudioPipeline.from_pretrained(\n",
    "    AUDIO_MODEL_PATH, scheduler=scheduler\n",
    ").to(device)\n",
    "\n",
    "model.scheduler.set_timesteps(TOTAL_STEPS)\n",
    "model.scheduler.timesteps = model.scheduler.timesteps.to(device)\n",
    "model.scheduler.alphas_cumprod = model.scheduler.alphas_cumprod.to(device)\n",
    "\n",
    "model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f9f94ca-3643-495f-9717-fa617c2f4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDIMScheduler, DDIMInverseScheduler\n",
    "\n",
    "# Your helper functions\n",
    "# from helpers import load_audio_to_numpy, load_audio\n",
    "# import ptp_utils\n",
    "\n",
    "class NullInversion:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tokenizer = model.tokenizer\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "        self.global_states = None\n",
    "        self.cond_embed = None\n",
    "        self.device = device\n",
    "        self.vae_scale_factor = 0.3704\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(\n",
    "            uncond_input.input_ids.to(self.model.device)\n",
    "        )[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(\n",
    "            text_input.input_ids.to(self.model.device)\n",
    "        )[0]\n",
    "        # print(f\"{uncond_embeddings.shape=}\")\n",
    "        # print(f\"{text_embeddings.shape=}\")\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def get_v_prediction(self, latents, t, context, global_states):\n",
    "        # Helper to get the model's v-prediction\n",
    "        timestep = t.expand(latents.shape[0])\n",
    "        v_pred = self.model.transformer(\n",
    "            latents, timestep, \n",
    "            encoder_hidden_states=context, \n",
    "            global_hidden_states=global_states,\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "        # print(\"v_pred\",v_pred.shape)\n",
    "        return v_pred\n",
    "\n",
    "    def get_noise_pred(self, latents, t, is_forward=True, context=None):\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        self.global_states = self.global_states.repeat(2, 1, 1)\n",
    "        if context is None:\n",
    "            context = self.context\n",
    "        guidance_scale = 1 if is_forward else GUIDANCE_SCALE\n",
    "        timestep = t.expand(latents.shape[0])\n",
    "        # print(timestep.shape)\n",
    "        noise_pred = self.model.transformer(latents_input, timestep, encoder_hidden_states=context,global_hidden_states=self.global_states)[0]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "            noise_prediction_text - noise_pred_uncond\n",
    "        )\n",
    "        if is_forward:\n",
    "            latents = self.next_step(noise_pred, t, latents)\n",
    "        else:\n",
    "            latents = self.prev_step(noise_pred, t, latents)\n",
    "        return latents\n",
    "\n",
    "    def next_step(\n",
    "        self,\n",
    "        model_output: torch.Tensor,  # this is v_t\n",
    "        timestep: int,\n",
    "        sample: torch.Tensor,        # this is x_t\n",
    "        return_orig: bool = False,\n",
    "    ):\n",
    "        dt = self.model.scheduler.num_train_timesteps // TOTAL_STEPS\n",
    "        max_step = self.model.scheduler.num_train_timesteps - 1\n",
    "    \n",
    "        # clamp to [0, max_step]\n",
    "        t = int(timestep)\n",
    "        t_next = min(t + dt, max_step)\n",
    "    \n",
    "        # grab alphas\n",
    "        alpha_t      = self.model.scheduler.alphas_cumprod[t]\n",
    "        alpha_t_next = self.model.scheduler.alphas_cumprod[t_next]\n",
    "        beta_t       = 1 - alpha_t\n",
    "    \n",
    "        # recover x0 and eps from v\n",
    "        x0  = alpha_t**0.5 * sample - beta_t**0.5 * model_output\n",
    "        eps = alpha_t**0.5 * model_output + beta_t**0.5 * sample\n",
    "    \n",
    "        # DDIM forward (inversion) update\n",
    "        next_sample = alpha_t_next**0.5 * x0 + (1 - alpha_t_next)**0.5 * eps\n",
    "    \n",
    "        if return_orig:\n",
    "            return next_sample, x0\n",
    "        return next_sample\n",
    "\n",
    "    def prev_step(\n",
    "        self,\n",
    "        model_output: torch.Tensor,  # this is v_t\n",
    "        timestep: int,\n",
    "        sample: torch.Tensor,        # this is x_t\n",
    "    ):\n",
    "        # ---- 1) figure out your \"backward\" timestep decrement ----\n",
    "        dt = self.model.scheduler.num_train_timesteps // TOTAL_STEPS\n",
    "        t_prev = max(timestep - dt, 0)\n",
    "    \n",
    "        # ---- 2) grab alphas ----\n",
    "        alpha_t      = self.model.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_t_prev = (\n",
    "            self.model.scheduler.alphas_cumprod[t_prev]\n",
    "            if t_prev >= 0\n",
    "            else self.model.scheduler.final_alpha_cumprod\n",
    "        )\n",
    "        beta_t = 1 - alpha_t\n",
    "    \n",
    "        # ---- 3) predict x0 and eps from v ----\n",
    "        x0  = alpha_t**0.5 * sample - beta_t**0.5 * model_output\n",
    "        eps = alpha_t**0.5 * model_output + beta_t**0.5 * sample\n",
    "    \n",
    "        # ---- 4) step backward ----\n",
    "        prev_sample = alpha_t_prev**0.5 * x0 + (1 - alpha_t_prev)**0.5 * eps\n",
    "    \n",
    "        return prev_sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2audio(self, latents: torch.FloatTensor, return_type: str = \"np\"):\n",
    "        \"\"\"\n",
    "        Decode a latent back into a waveform.\n",
    "        – latents: (B, C_latent, T_latent)\n",
    "        – returns either a torch.Tensor (B, C_audio, T_audio)\n",
    "          or a numpy array (T_audio, C_audio) for the first batch item.\n",
    "        \"\"\"\n",
    "        # 1) undo the scaling\n",
    "        latents = latents.detach() / self.vae_scale_factor\n",
    "        # 2) decode\n",
    "        audio = self.model.vae.decode(latents)[\"sample\"]\n",
    "        # audio is now a torch.Tensor of shape (B, C, T)\n",
    "        if return_type == \"np\":\n",
    "            # clamp into valid [-1,1] range and convert\n",
    "            audio = audio.clamp(-1, 1)\n",
    "            # move channels axis last and pick batch 0\n",
    "            audio_np = audio.cpu().permute(0, 2, 1).numpy()[0]\n",
    "            return audio_np\n",
    "        return audio\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def audio2latent(self, audio: Union[np.ndarray, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Encode a waveform into VAE latents.\n",
    "        – audio: either a np.ndarray (T, C) or torch.Tensor (C, T) / (B, C, T)\n",
    "        – returns a torch.FloatTensor of latents (B, C_latent, T_latent)\n",
    "        \"\"\"\n",
    "        # 1) to torch\n",
    "        # if isinstance(audio, np.ndarray):\n",
    "        #     # assume shape (T, C) → (C, T)\n",
    "        #     audio = torch.from_numpy(audio).permute(1, 0)\n",
    "        # now audio is Tensor of shape (C, T) or (B, C, T)\n",
    "        if audio.dim() == 2:\n",
    "            audio = audio.unsqueeze(0)    # → (1, C, T)\n",
    "        audio = audio.to(self.device).float()\n",
    "        # 2) encode\n",
    "        latents = self.model.vae.encode(audio)[\"latent_dist\"].mean\n",
    "        # 3) apply the same scale factor\n",
    "        latents = latents * self.vae_scale_factor\n",
    "        return latents\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent, audio_tensor):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        all_latent = [latent]\n",
    "        latent = latent.clone().detach()\n",
    "\n",
    "        #before we can call the transformer for the prediction, we will have to calculate global states by projection \n",
    "        # We will get audio length information and and add it to the global state projection\n",
    "\n",
    "        duration_s = audio_tensor.shape[-1] / self.model.vae.sampling_rate\n",
    "        ss, es = self.model.encode_duration(0.0, duration_s, self.model.device, do_classifier_free_guidance=False, batch_size=1)\n",
    "        self.global_states = self.model.transformer.global_proj(torch.cat([ss, es], dim=-1))\n",
    "        # print(\"globalstates\", self.global_states)\n",
    "        for i in range(NUM_DDIM_STEPS):\n",
    "            t = self.model.scheduler.timesteps[\n",
    "                len(self.model.scheduler.timesteps) - i - 1\n",
    "            ]\n",
    "            noise_pred = self.get_v_prediction(latent, t, cond_embeddings, self.global_states)\n",
    "            latent = self.next_step(noise_pred, t, latent)\n",
    "            all_latent.append(latent)\n",
    "        return all_latent\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, audio_tensor):\n",
    "        latent = self.audio2latent(audio_tensor)\n",
    "        print(\"after latent\")\n",
    "        # # print(f\"latent shape before ddim loop {latent.shape}\")\n",
    "        audio_rec = self.latent2audio(latent)\n",
    "        print(\"after latent to audio\")\n",
    "        ddim_latents = self.ddim_loop(latent, audio_tensor)\n",
    "        print(\"in loop\")\n",
    "        # print(f\"{len(ddim_latents)=}\")\n",
    "        return audio_rec, ddim_latents\n",
    "\n",
    "    def null_optimization(self, latents, num_inner_steps, epsilon):\n",
    "        uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "        uncond_embeddings_list = []\n",
    "        latent_cur = latents[-1]\n",
    "        for i in tqdm(range(NUM_DDIM_STEPS)):\n",
    "            uncond_embeddings = uncond_embeddings.clone().detach()\n",
    "            uncond_embeddings.requires_grad = True\n",
    "            optimizer = Adam([uncond_embeddings], lr=1e-2 * (1.0 - i / 100.0))\n",
    "            latent_prev = latents[len(latents) - i - 2]\n",
    "            t = self.model.scheduler.timesteps[i]\n",
    "            with torch.no_grad():\n",
    "                # print(latent_cur.shape, cond_embeddings.shape, self.global_states.shape,  \"no 1 prediction\")\n",
    "                if latent_cur.shape[0] < 2:\n",
    "                    self.global_states = self.global_states[:1]\n",
    "                    \n",
    "                noise_pred_cond = self.get_v_prediction(\n",
    "                    latent_cur, t, cond_embeddings, self.global_states\n",
    "                )\n",
    "            for j in range(num_inner_steps):\n",
    "                # print(latent_cur.shape, cond_embeddings.shape, self.global_states.shape, \"no 2 prediction\")\n",
    "                noise_pred_uncond = self.get_v_prediction(\n",
    "                    latent_cur, t, uncond_embeddings, self.global_states\n",
    "                )\n",
    "                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (\n",
    "                    noise_pred_cond - noise_pred_uncond\n",
    "                )\n",
    "                latents_prev_rec = self.prev_step(noise_pred, t, latent_cur)\n",
    "                loss = F.mse_loss(latents_prev_rec, latent_prev)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                if loss_item < epsilon + i * 2e-5:\n",
    "                    break\n",
    "            uncond_embeddings_list.append(uncond_embeddings[:1].detach())\n",
    "            with torch.no_grad():\n",
    "                context = torch.cat([uncond_embeddings, cond_embeddings])\n",
    "                latent_cur = self.get_noise_pred(latent_cur, t, False, context)\n",
    "        return uncond_embeddings_list\n",
    "        \n",
    "    # @torch.no_grad()\n",
    "    def invert(\n",
    "        self,\n",
    "        audio_path: str,\n",
    "        prompt: str,\n",
    "        num_inner_steps=10,\n",
    "        early_stop_epsilon=1e-5,\n",
    "        verbose=False\n",
    "    ):\n",
    "        print(\"first here\")\n",
    "        self.init_prompt(prompt)\n",
    "        ptp_utils.register_attention_control(self.model, None)\n",
    "        audio_gt = load_audio(audio_path) # Downsampling audio with 2 channels\n",
    "        # audio_gt = load_audio(audio_path, device=device)\n",
    "        print(f\"audio after loading to numpy {audio_gt.shape}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"DDIM inversion...\")\n",
    "        audio_rec, ddim_latents = self.ddim_inversion(audio_gt)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Null-text optimization...\")\n",
    "        uncond_embeddings = self.null_optimization(\n",
    "            ddim_latents, num_inner_steps, early_stop_epsilon\n",
    "        )\n",
    "        return (audio_gt, audio_rec), ddim_latents, ddim_latents[-1], uncond_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def invert_style(\n",
    "        self,\n",
    "        audio: torch.Tensor,\n",
    "        prompt,\n",
    "        num_inference_steps=100,\n",
    "        guidance_scale=7.0,\n",
    "        eta=0.0,\n",
    "        return_intermediates=False,\n",
    "        **kwds,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        invert a real audio into noise map with determinisc DDIM inversion\n",
    "        \"\"\"\n",
    "        DEVICE = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available(\n",
    "            ) else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "\n",
    "        batch_size = audio.shape[0]\n",
    "        if isinstance(prompt, list):\n",
    "            if batch_size == 1:\n",
    "                audio = audio.expand(len(prompt), -1, -1)\n",
    "        elif isinstance(prompt, str):\n",
    "            if batch_size > 1:\n",
    "                prompt = [prompt] * batch_size\n",
    "\n",
    "        # text embeddings\n",
    "        text_input = self.tokenizer(\n",
    "            prompt, padding=\"max_length\", max_length=64, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(DEVICE))[0]\n",
    "        # print(text_embeddings.shape) # these are the same as hjidden dimension [1,64,768]\n",
    "        \n",
    "        # define initial latents\n",
    "        latents = self.audio2latent(audio)\n",
    "        # print(latents.shape)# should be [1,64,107]\n",
    "        \n",
    "        start_latents = latents\n",
    "        # unconditional embedding for classifier free guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            unconditional_input = self.tokenizer(\n",
    "                [\"\"] * batch_size,\n",
    "                padding=\"max_length\",\n",
    "                max_length=64,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            unconditional_embeddings = self.model.text_encoder(\n",
    "                unconditional_input.input_ids.to(DEVICE)\n",
    "            )[0]\n",
    "            text_embeddings = torch.cat(\n",
    "                [unconditional_embeddings, text_embeddings], dim=0\n",
    "            )\n",
    "        # interative sampling\n",
    "        self.model.scheduler.set_timesteps(num_inference_steps)\n",
    "        latents_list = [latents]\n",
    "        pred_x0_list = [latents]\n",
    "        for i, t in enumerate(\n",
    "            tqdm(reversed(self.model.scheduler.timesteps), desc=\"DDIM Inversion\")\n",
    "        ):\n",
    "            if guidance_scale > 1.0:\n",
    "                model_inputs = torch.cat([latents] * 2)\n",
    "            else:\n",
    "                model_inputs = latents\n",
    "\n",
    "            # Calcualte global states, length\n",
    "            duration_s = audio.shape[-1] / self.model.vae.sampling_rate\n",
    "            ss, es = self.model.encode_duration(0.0, duration_s, self.model.device, do_classifier_free_guidance=False, batch_size=1)\n",
    "            self.global_states = self.model.transformer.global_proj(torch.cat([ss, es], dim=-1))\n",
    "            # print(self.global_states.shape)\n",
    "\n",
    "            if model_inputs.shape[0] > 1:\n",
    "                self.global_states = self.global_states.repeat(2, 1, 1) #[2, 1, 1536])\n",
    "\n",
    "            timestep = t.expand(latents.shape[0]).to(self.model.device)\n",
    "            # print(f\"{model_inputs.shape=}\")\n",
    "            # print(f\"{text_embeddings.shape=}\")\n",
    "            # print(f\"{self.global_states.shape=}\") \n",
    "            # predict the noise\n",
    "            noise_pred = self.model.transformer(\n",
    "                model_inputs, timestep, encoder_hidden_states=text_embeddings, global_hidden_states=self.global_states\n",
    "            )[0]\n",
    "\n",
    "            # print(noise_pred.shape) #[2, 64, 107])\n",
    "     \n",
    "            if guidance_scale > 1.0:\n",
    "                noise_pred_uncon, noise_pred_con = noise_pred.chunk(2, dim=0)\n",
    "                noise_pred = noise_pred_uncon + guidance_scale * (\n",
    "                    noise_pred_con - noise_pred_uncon\n",
    "                )\n",
    "            # compute the previous noise sample x_t-1 -> x_t\n",
    "            latents, pred_x0 = self.next_step(noise_pred, t, latents, True)\n",
    "            latents_list.append(latents)\n",
    "            pred_x0_list.append(pred_x0)\n",
    "\n",
    "        if return_intermediates:\n",
    "            # return the intermediate laters during inversion\n",
    "            return latents, latents_list\n",
    "        return latents, start_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c221992-4adf-442b-8a81-f67be997eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_inversion = NullInversion(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480b3b3c-b2ea-449b-8495-304de27c26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# editor = ReweightCrossAttentionControl(\n",
    "#     # start_step=int(0.2*TOTAL_STEPS),\n",
    "#     # end_step=int(0.8*TOTAL_STEPS),\n",
    "#     # start_layer=0,\n",
    "#     # end_layer=20,\n",
    "#     total_steps=TOTAL_STEPS,\n",
    "#     # style_scale = 5.0\n",
    "# )\n",
    "# register_attention_editor_diffusers(model, editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde451d-65c3-4655-b9f1-a8745ee7a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_files = get_audio_files(\"c\")\n",
    "style_files = get_audio_files(\"s\")\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "for cpath in content_files:\n",
    "    print(f\"Processing content: {os.path.basename(cpath)}\")\n",
    "    pkl = cpath.rsplit('.',1)[0] + \".pkl\"\n",
    "\n",
    "    source_prompt = \"\"\n",
    "    target_prompt = \"\"\n",
    "    prompts = [source_prompt, target_prompt]\n",
    "\n",
    "    if os.path.isfile(pkl):\n",
    "        (audio_gt, audio_rec),cont_trajectory, content_latent_vector, u_c = pickle.load(open(pkl,'rb'))\n",
    "\n",
    "        sf.write('content_recon.wav', audio_rec.squeeze(), TARGET_SR)\n",
    "        test_rec = null_inversion.latent2audio(content_latent_vector)\n",
    "        wav_noise = test_rec.squeeze()\n",
    "        sf.write('last_recon.wav', wav_noise, TARGET_SR)\n",
    "        load_and_view_audio('last_recon.wav')\n",
    "\n",
    "        \n",
    "        print(\"Ground Truth\")\n",
    "        load_and_view_audio(\"c/content.wav\")\n",
    "        print(\"\\n# ********************\\n\")\n",
    "\n",
    "    else:\n",
    "        # with torch.no_grad():\n",
    "        (audio_gt, audio_rec),cont_trajectory, content_latent_vector, u_c = null_inversion.invert(\n",
    "            cpath, prompts, verbose=True\n",
    "        )\n",
    "        # print(\"content latent vector\",content_latent_vector.shape)\n",
    "\n",
    "        print(\"Ground Truth\")\n",
    "        load_and_view_audio(\"c/content.wav\")\n",
    "        print(\"\\n# ********************\\n\")\n",
    "        print(\"Reconstruction\")\n",
    "        test_rec = null_inversion.latent2audio(content_latent_vector)\n",
    "        wav_noise = test_rec.squeeze()\n",
    "        sf.write('last_recon.wav', wav_noise, TARGET_SR)\n",
    "        load_and_view_audio('last_recon.wav')\n",
    "        \n",
    "        pickle.dump(((audio_gt, audio_rec), cont_trajectory, content_latent_vector, u_c), open(pkl,'wb'))\n",
    "        print(\"Saved content reconstruction: {content_recon.wav\")\n",
    "\n",
    "    start_code_content = content_latent_vector.expand(len(prompts), -1, -1)\n",
    "    for style_path in style_files:\n",
    "        style_audio = load_audio(style_path)\n",
    "        final_latent, style_latent_list = null_inversion.invert_style(\n",
    "            style_audio,\n",
    "            source_prompt,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            num_inference_steps=TOTAL_STEPS,\n",
    "            return_intermediates=True,\n",
    "        )\n",
    "        test_rec_style = null_inversion.latent2audio(final_latent)\n",
    "        wav_noise_style = test_rec_style.squeeze()\n",
    "        sf.write('last_recon_style.wav', wav_noise_style, TARGET_SR)\n",
    "        load_and_view_audio('last_recon_style.wav')\n",
    "\n",
    "        # Basically test this \n",
    "        # editor = ReweightCrossAttentionControl(\n",
    "        #     # start_step=int(0.2*TOTAL_STEPS),\n",
    "        #     # end_step=int(0.8*TOTAL_STEPS),\n",
    "        #     # start_layer=0,\n",
    "        #     # end_layer=20,\n",
    "        #     total_steps=TOTAL_STEPS,\n",
    "        #     # style_scale = 5.0\n",
    "        # )\n",
    "        # register_attention_editor_diffusers(model, editor)\n",
    "\n",
    "        # Audio Mix :\n",
    "        aud_stylized = model(\n",
    "            prompts,\n",
    "            latents=start_code_content,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            uncond_embeddings=u_c,\n",
    "            num_inference_steps=TOTAL_STEPS,\n",
    "            ref_intermediate_latents=[cont_trajectory, style_latent_list]\n",
    "         )\n",
    "\n",
    "        audio_np = aud_stylized.detach().cpu().numpy()[1].T\n",
    "        audio_np_style = aud_stylized.detach().cpu().numpy()[0].T\n",
    "        sf.write(\"mixed.wav\", audio_np, TARGET_SR)\n",
    "        sf.write(\"style_from_model.wav\", audio_np_style, TARGET_SR)\n",
    "\n",
    "        \n",
    "        load_and_view_audio(\"mixed.wav\")\n",
    "        print(\"****************** ************************\")\n",
    "        load_and_view_audio(\"style_from_model.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068661e-734b-4cac-80b6-d560e3eb7677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
